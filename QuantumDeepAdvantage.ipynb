{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Link](https://github.com/dacozai/QuantumDeepAdvantage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install qiskit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from QuantumDeepAdvantage.Environment.modules import *\n",
    "from qiskit import IBMQ\n",
    "from qiskit.providers import aer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \n",
    "    \"\"\"\n",
    "    State: Wavefunction, 1 x 2^n complex vector.\n",
    "    Rewards: \n",
    "        - If Steps = 100\n",
    "            Return |<psi0|psif>|^2 / #steps\n",
    "        - If |<psi0|psif>|^2 = 1, \n",
    "            Return 1 / #steps.\n",
    "        \n",
    "        Only give reward when measure\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_qubits):\n",
    "        self.MAXIMUM_MOVE = 10 \n",
    "        self.MINIMAL_VALUE = 10e-6\n",
    "        self.provider = aer.aerprovider.AerProvider()\n",
    "        self.backend = self.provider.get_backend('statevector_simulator')\n",
    "        self.num_qubits = num_qubits\n",
    "        \n",
    "        self.target_state = np.array([1/np.sqrt(2), 0, 0, 1/np.sqrt(2)])\n",
    "        self.reset()\n",
    "            \n",
    "    def reset(self):\n",
    "        #reset the state to the initial state after every step\n",
    "        self.steps = 0\n",
    "        self.state = np.zeros(2**self.num_qubits)\n",
    "        self.state[0] = 1\n",
    "        \n",
    "        self.inner_product = np.abs(np.vdot(self.state, self.target_state))**2\n",
    "            \n",
    "    def step(self, action):\n",
    "        # apply step on qubit\n",
    "        qc = QuantumCircuit(2)\n",
    "        qc.initialize(self.state, [0, 1])\n",
    "        # apply X gate on qubit action[1]\n",
    "        if action[0] == \"X\":\n",
    "            qc.x(action[1])\n",
    "        elif action[0] == \"Y\":\n",
    "            qc.y(action[1])\n",
    "        elif action[0] == \"Z\":\n",
    "            qc.z(action[1])\n",
    "        elif action[0] == \"T\":\n",
    "            qc.t(action[1])\n",
    "        elif action[0] == \"H\":\n",
    "            qc.h(action[1])\n",
    "        elif action[0] == \"CX\":\n",
    "            qc.cx(action[1][0], action[1][1])\n",
    "        elif action[0] == \"CCX\":\n",
    "            qc.ccx(action[1][0], action[1][1], action[1][2])\n",
    "            \n",
    "        qobj = assemble(qc)\n",
    "        job = self.backend.run(qobj)\n",
    "        result = job.result()        \n",
    "        self.state = np.array(result.data()['statevector'])\n",
    "        self.state = self.state[:,0] + 1j * self.state[:,1]\n",
    "        \n",
    "        self.inner_product = self.inner_product_measure() #calculate the distance between initial state and target state\n",
    "        \n",
    "        print('At end of step {}, action is {}, inner_product is {}'.format(self.steps, action, self.inner_product))\n",
    "        \n",
    "        self.steps += 1\n",
    "        \n",
    "    def inner_product_measure(self):\n",
    "        return np.abs(np.vdot(self.state, self.target_state))**2\n",
    "\n",
    "    def trace_distance_measure(self):\n",
    "        \n",
    "        state_density = np.kron(np.array([self.state]).conjugate().transpose(), self.state)\n",
    "        target_density = np.kron(np.array([self.target_state]).conjugate().transpose(), self.target_state)\n",
    "\n",
    "        diff_density = target_density - state_density\n",
    "        \n",
    "        diff_eigs = np.linalg.eigvals(diff_density)\n",
    "        \n",
    "        trace_distance = 0.5 * sum(np.abs(diff_eigs))\n",
    "        return trace_distance\n",
    "        \n",
    "    \n",
    "    def reward(self):\n",
    "        \n",
    "        if(np.abs(self.inner_product - 1) < self.MINIMAL_VALUE):\n",
    "            # return 1 / self.steps\n",
    "            return 100 / self.steps\n",
    "        elif self.steps == self.MAXIMUM_MOVE:\n",
    "            # return ( self.inner_product - 1) / self.steps\n",
    "            return -1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def is_terminated(self):\n",
    "        if (np.abs(self.inner_product - 1) < 10e-6) or self.steps == self.MAXIMUM_MOVE:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!d:\\installation\\anaconda3\\python.exe -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, optimizers\n",
    "\n",
    "from typing import Dict, Tuple, Sequence, List\n",
    "import copy\n",
    "\n",
    "from QuantumDeepAdvantage.Agent.network.nets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dqn:\n",
    "    \"\"\"\n",
    "    Deep Q Network\n",
    "    Action Space: {x1, x2, y1, y2, z1, z2, h1, h2, c12, c21}\n",
    "    Attribute\n",
    "    self.num_qubits: \n",
    "    self.input_dim:\n",
    "    Methods\n",
    "    parse_action: convert 0 to 9 to specific gate and its argument\n",
    "    \"\"\"\n",
    "    def __init__(self, num_qubits=2, num_action=12, gamma=0.99, alpha=10e-2, epsilon=0.01):\n",
    "        self.num_qubits = num_qubits\n",
    "        self.input_sz = 2 ** self.num_qubits \n",
    "        self.input_dim = ( 1, self.input_sz )\n",
    "        self.num_action = num_action \n",
    "        self.init = False\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.net_instance = vanila_neural_net(self.input_sz, self.num_action, self.input_dim, self.alpha)\n",
    "        self.q_network = self.net_instance.init_model()\n",
    "\n",
    "        self.total_reward = 0\n",
    "        self.win_times = 0\n",
    "    \n",
    "    def parse_action(self, action_num):\n",
    "        if action_num == 0 or action_num == 1:\n",
    "            return [\"X\", action_num]\n",
    "        elif action_num == 2 or action_num == 3:\n",
    "            return [\"Y\", action_num%self.num_qubits]\n",
    "        elif action_num == 4 or action_num == 5:\n",
    "            return [\"Z\", action_num%self.num_qubits]\n",
    "        elif action_num == 6 or action_num == 7:\n",
    "            return [\"H\", action_num%self.num_qubits]\n",
    "        elif action_num == 8 or action_num == 9:\n",
    "            return [\"T\", action_num%self.num_qubits]\n",
    "\n",
    "        # It can be better!!! (Only good in 2 qubits)\n",
    "        return [ \"CX\", [action_num%self.num_qubits, 1-(action_num%self.num_qubits)] ]\n",
    "  \n",
    "    def find_max_val_indx(self, q_values):\n",
    "        init_flag = False\n",
    "        indx_list = []\n",
    "        max_val:float = None\n",
    "        for indx in range(self.num_action):\n",
    "            if not init_flag:\n",
    "                max_val = q_values[indx] \n",
    "                indx_list.append(indx)\n",
    "                init_flag = True\n",
    "            else:\n",
    "                if max_val < q_values[indx]:\n",
    "                    max_val = q_values[indx]\n",
    "                    indx_list = [indx]\n",
    "                elif max_val == q_values[indx]:\n",
    "                    indx_list.append(indx)\n",
    "    \n",
    "        return np.random.choice(indx_list) \n",
    "\n",
    "    def get_action(self, state):\n",
    "        self.prev_state = copy.deepcopy(state.reshape(1, self.input_sz))\n",
    "        favor_action = None\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            favor_action = np.random.choice(range(self.num_action))\n",
    "        else:\n",
    "            q_values = self.q_network.predict(self.prev_state)[0]\n",
    "            favor_action = self.find_max_val_indx(q_values)\n",
    "\n",
    "        self.prev_action = favor_action\n",
    "        return self.parse_action(favor_action)\n",
    "\n",
    "    def learn_from_transition(self, next_state, reward, terminate):\n",
    "        if not self.init:\n",
    "            self.init = True\n",
    "        return\n",
    "\n",
    "        state = self.prev_state\n",
    "        n_state = copy.deepcopy(next_state.reshape(self.input_dim))\n",
    "        action = self.prev_action\n",
    "        q_table = self.q_network.predict(state)\n",
    "\n",
    "        q_values = 0\n",
    "        if not terminate:\n",
    "            q_values = np.max(q_table[0])\n",
    "            # print(\"q_values is \",q_values)\n",
    "        else:\n",
    "            self.init = False\n",
    "            self.prev_action = None\n",
    "            self.prev_state = None\n",
    "\n",
    "        q_table[0][action] = reward + self.gamma * q_values\n",
    "        self.q_network.fit(state, q_table, batch_size=1, verbose=0)\n",
    "\n",
    "    def reset(self):\n",
    "        self.init = False\n",
    "        self.q_network = self.net_instance.init_model()\n",
    "        # self.q_network.save_weights(filepath +'train_' + str(ag_times) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class drqn(dqn):\n",
    "\n",
    "  def __init__(self, num_qubits, gamma=0.9, alpha=10e-2):\n",
    "    super().__init__(num_qubits=num_qubits, gamma=gamma, alpha=alpha)\n",
    "    self.net_instance = lstm(self.input_sz, self.num_action, self.input_dim, self.alpha)\n",
    "    self.q_network = self.net_instance.init_model()\n",
    "\n",
    "  # convert 1 * 2^n array into 2 * 2^n array\n",
    "  def complexToReal(self, complexArray):\n",
    "    return np.array([[[complexArray.real[indx], complexArray.imag[indx]] for indx in range(len(complexArray))]])\n",
    "\n",
    "  def get_action(self, state):\n",
    "    self.prev_state = copy.deepcopy(self.complexToReal(state))\n",
    "    favor_action = None\n",
    "    if np.random.uniform(0, 1) < self.epsilon:\n",
    "      favor_action = np.random.choice(range(self.num_action))\n",
    "    else:\n",
    "      q_values = self.q_network.predict(self.prev_state)[0]\n",
    "      favor_action = self.find_max_val_indx(q_values)\n",
    "\n",
    "    self.prev_action = favor_action\n",
    "    return self.parse_action(favor_action)\n",
    "\n",
    "  def learn_from_transition(self, next_state, reward, terminate):\n",
    "    if not self.init:\n",
    "      self.init = True\n",
    "      return\n",
    "\n",
    "    state = self.prev_state\n",
    "    n_state = copy.deepcopy(self.complexToReal(next_state))\n",
    "    action = self.prev_action\n",
    "    q_table = self.q_network.predict(state)\n",
    "\n",
    "    q_values = 0\n",
    "    if not terminate:\n",
    "      q_values = np.max(q_table[0])\n",
    "    else:\n",
    "      self.init = False\n",
    "      self.prev_action = None\n",
    "      self.prev_state = None\n",
    "\n",
    "    q_table[0][action] = reward + self.gamma * q_values\n",
    "    self.q_network.fit(state, q_table, batch_size=1, verbose=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
